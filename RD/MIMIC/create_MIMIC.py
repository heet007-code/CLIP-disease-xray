# -*- coding: utf-8 -*-
"""RDD

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FxphpNJ7ztFjzhTARJnmRD730j_LOD-9
"""

# Install PhysioNet client
!pip install physionet-dataloader

# Download CXR-LT label files
!wget -r -N -c -np --user username --ask-password https://physionet.org/files/cxr-lt-iccv-workshop-cvamd/2.0.0/
!unzip -o cxr-lt-iccv-workshop-cvamd-2.0.0.zip -d cxr-lt

import pandas as pd

# Load the CXR-LT label CSV
labels = pd.read_csv("physionet.org/files/cxr-lt-iccv-workshop-cvamd/2.0.0/cxr-lt-2024/labels.csv")

# ‚úÖ List of all rare disease labels you want
rare_labels = [
    'Azygos Lobe',
    'Hydropneumothorax',
    'Pulmonary Embolism',
    'Pulmonary Hypertension',
    'Round Atelectasis',
    'Tuberculosis',
    'Cardiomyopathy',
]


# Check which labels from your list actually exist in the dataset
available_labels = [label for label in rare_labels if label in labels.columns]
print("Available rare labels in dataset:", available_labels)

# Filter rows where any of those labels is positive (i.e., 1)
rare_df = labels[labels[available_labels].sum(axis=1) > 0]

# View the first few rows
print(rare_df[['subject_id', 'study_id', 'dicom_id'] + available_labels].head())

# Optional: save filtered list to CSV
rare_df.to_csv("rare_disease_cxr_subset.csv", index=False)

def get_download_url(row):
    subject_id = str(row['subject_id'])
    study_id = row['study_id']
    dicom_id = row['dicom_id']
    folder = f"p{subject_id[:2]}/p{subject_id}/{study_id}/"
    return f"https://physionet.org/files/mimic-cxr-jpg/2.0.0/files/{folder}{dicom_id}.jpg"

# Add download URL column
rare_df['download_url'] = rare_df.apply(get_download_url, axis=1)

# View a few sample URLs
print(rare_df[['dicom_id', 'download_url']].head())

# Optional: save for later
rare_df.to_csv("rare_disease_download_list.csv", index=False)

# Save just the URLs into a text file (one URL per line)
rare_df['download_url'].to_csv('urls.txt', index=False, header=False)

!apt-get install wget

# Create a folder to save images
!mkdir -p rare_disease_xrays

# Download all images (you will be prompted for password ONCE)
!wget --user=username --ask-password -i urls.txt -P rare_disease_xrays/

for label in available_labels:
    count = (rare_df[label] == 1).sum()
    print(f"{label}: {count} positive samples")

import os
import pandas as pd
import numpy as np
import shutil
from tqdm import tqdm

# Load rare_df
rare_df = pd.read_csv("rare_disease_download_list.csv")

# Labels you're working with
rare_labels = [
    'Azygos Lobe',
    'Hydropneumothorax',
    'Pulmonary Embolism',
    'Pulmonary Hypertension',
    'Tuberculosis',
    'Cardiomyopathy',
]

# Set seed for reproducibility
np.random.seed(42)

# Create base directory
base_dir = "few_shot"
os.makedirs(base_dir, exist_ok=True)

# Local images folder (already set for you)
local_images_path = "rare_disease_xrays"

# List of available dicom_ids
available_images = set([fname.replace(".jpg", "") for fname in os.listdir(local_images_path) if fname.endswith(".jpg")])

print(f"üîé Found {len(available_images)} available images locally.")

# Filter rare_df to only keep rows that exist locally
rare_df = rare_df[rare_df['dicom_id'].isin(available_images)]
print(f"‚úÖ {len(rare_df)} entries in rare_df after filtering for available images.")

# Prepare split containers
few_shot_splits = {}

# Keep track of used dicom_ids
used_ids = set()

# Helper to copy a single image
def copy_image(dicom_id, save_path):
    source_file = os.path.join(local_images_path, dicom_id + ".jpg")
    if os.path.exists(source_file):
        shutil.copy(source_file, save_path)
    else:
        print(f"‚ùóCould not find {dicom_id}.jpg locally!")

# Few-shot creation loop
for shots in [1, 2, 4, 8, 16]:
    selected_rows = []

    for label in rare_labels:
        positive_rows = rare_df[(rare_df[label] == 1) & (~rare_df['dicom_id'].isin(used_ids))]

        if len(positive_rows) < shots:
            raise ValueError(f"Not enough positive samples for {label} to create {shots}-shot set.")

        sampled = positive_rows.sample(n=shots, replace=False)
        used_ids.update(sampled['dicom_id'].tolist())
        selected_rows.append(sampled)

    few_shot_df = pd.concat(selected_rows).drop_duplicates()
    few_shot_splits[f"{shots}_shot"] = few_shot_df

    # Save CSV
    split_dir = os.path.join(base_dir, f"{shots}_shot")
    os.makedirs(split_dir, exist_ok=True)
    few_shot_df.to_csv(os.path.join(split_dir, f"few_shot_{shots}_per_disease.csv"), index=False)

    print(f"‚úÖ {shots}-shot split created with {len(few_shot_df)} images.")

    # Copy images
    print(f"üìÇ Copying images for {shots}-shot split...")
    for idx, row in tqdm(few_shot_df.iterrows(), total=len(few_shot_df)):
        dicom_id = row['dicom_id']
        filename = f"{dicom_id}.jpg"
        save_path = os.path.join(split_dir, filename)

        if not os.path.exists(save_path):
            copy_image(dicom_id, save_path)

    print(f"‚úÖ All images copied for {shots}-shot split.\n")

print("üéâ All few-shot datasets and images organized locally!")

import shutil

# Zip the entire 'few_shot' folder
shutil.make_archive('few_shot_dataset', 'zip', 'few_shot')

print("‚úÖ few_shot_dataset.zip created!")